<html>
<head>
<title>
Workshop on Learning-Augmented Algorithms	
</title>

	<script language="JavaScript" type="text/JavaScript">
	function toggle(id) {
	if (document.getElementById){
		if (document.getElementById(id).style.display == "none"){
			document.getElementById(id).style.display = "flex";
			}
		else{
			document.getElementById(id).style.display = "none";
			}
		}
	}
	</script>
</head>
<body>
<h2>
<r>
Workshop on Learning-Augmented Algorithms
</r>
</h2>
<h3>
<r>
Date: August 19-21, 2024</br>
Location: TTIC, Chicago, IL</br>
Address: 6045 S Kenwood Ave, Chicago, IL 60637
</r>
<section>
			<p align="left">
			<iframe loading="lazy"
					src="https://www.google.com/maps?q=Toyota+Technological+Institute+at+Chicago&#038;t=m&#038;z=14&#038;output=embed&#038;iwloc=near"
					title="TTIC"
					aria-label="TTIC"
					style="width:30%; height:200px;overflow:auto;"
			></iframe>
			</p>
</section>
<a href="https://samsonzhou.github.io/Getting-to-TTIC-Maps.pdf">Parking Information</a>		
</h3>
<h3>
Organizers:
</h3>
<ul>
<li><a href="https://people.csail.mit.edu/indyk/">Piotr Indyk</a>, MIT</li>
<li><a href="http://www.mit.edu/~vakilian/">Ali Vakilian</a>, TTIC</li>
<li><a href="https://samsonzhou.github.io/">Samson Zhou</a>, Texas A&#38;M</li>
</ul>
<h3>
Sponsors:
</h3>
This workshop is part of the <a href="https://www.ttic.edu/summer-workshop-2024/">2024 TTIC Summer Workshop Program</a> and is sponsored by generous support from TTIC and NSF. 
<h3>
Registration:
</h3>
<a href="https://docs.google.com/forms/d/e/1FAIpQLSecfgYl1s4dQgJt3nMr5vmKX6PhVGgCHHkorzcJWrIb0TTPHw/viewform">Registration Form</a>
</br>
</br>
<i>
We will be holding sessions for poster and lightning talks. 
If you are interested in presenting a poster or giving a lightning talk, please indicate your interest and preference in the registration form. 
While we aim to accommodate all submissions, we may have to limit the number of presentations due to time and space constraints
</i>
<h3>
Participants:
</h3>
<ul>
<li><a href="https://www.cs.tau.ac.il/~azar/">Yossi Azar</a> (Tel Aviv)</li>
<li><a href="https://users.cs.utah.edu/~bhaskara/">Aditya Bhaskara</a> (Utah)</li>
<li><a href="https://www.cs.cmu.edu/~ninamf/">Nina Balcan</a> (CMU)</li>
<li><a href="https://ericbalkanski.com/">Eric Balkanski</a> (Columbia)</li>
<li><a href="https://samidavies.com">Sami Davies</a> (Simons)</li>
<li><a href="https://www.mit.edu/~golrezae/">Negin Golrezaei</a> (MIT)</li>
<li><a href="https://cs.nyu.edu/~anupamg/">Anupam Gupta</a> (NYU)</li>
<li><a href="https://faculty.ucmerced.edu/sim3/">Sungjin Im</a> (UC Merced)</li>
<li><a href="https://people.csail.mit.edu/indyk/">Piotr Indyk</a> (MIT)</li>
<li><a href="https://people.csail.mit.edu/kraska/">Tim Kraska</a> (MIT)</li>
<li><a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a> (Google)</li>
<li><a href="https://quanquancliu.com/">Quanquan C. Liu</a> (Yale)</li>
<li><a href="https://www.eecs.harvard.edu/~michaelm/">Michael Mitzenmacher</a> (Harvard)</li>
<li><a href="https://www.andrew.cmu.edu/user/moseleyb/">Ben Moseley</a> (CMU)</li>
<li><a href="https://www.ccs.neu.edu/home/hlnguyen/">Huy L. Nguyen</a> (Northeastern)</li>
<li><a href="https://www.debmalyapanigrahi.org/">Debmalya Panigrahi</a> (Duke)</li>
<li><a href="https://adampolak.github.io/">Adam Polak</a> (Bocconi)</li>
<li><a href="https://barnasaha.net/">Barna Saha</a> (UC San Diego)</li>
<li><a href="http://www.mit.edu/~vakilian/">Ali Vakilian</a> (TTIC)</li>
<li><a href="https://theory.stanford.edu/~sergei/">Sergei Vassilvitskii</a> (Google)</li>
<li><a href="https://vitercik.github.io/">Ellen Vitercik</a> (Stanford)</li>
<li><a href="https://adamwierman.com/">Adam Wierman</a> (Caltech)</li>
<li><a href="https://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a> (CMU)</li>
<li><a href="https://samsonzhou.github.io/">Samson Zhou</a> (Texas A&#38;M)</li>
</ul>
<h3>
Schedule (Tentative):
</h3>
<u>Monday:</u></br>
9:30-9:50 Breakfast</br>
9:50-10 Opening Remarks</br>
10-10:30 <b>Michael Mitzenmacher</b>: SkipPredict: When to Invest in Predictions for Scheduling [<a href="javascript:toggle('Gupta')">abstract</a>]
<div id="Gupta" style="display: none">
<p id="abs">
In light of recent work on scheduling with predicted job sizes, we consider the effect of the cost of predictions
in queueing systems, removing the assumption in prior research that predictions are external to the system’s
resources and/or cost-free. In particular, we introduce a novel approach to utilizing predictions, SkipPredict,
designed to address their inherent cost. Rather than uniformly applying predictions to all jobs, we propose
a tailored approach that categorizes jobs based on their prediction requirements. To achieve this, we employ
one-bit “cheap predictions” to classify jobs as either short or long. SkipPredict prioritizes predicted short jobs
over long jobs, and for the latter, SkipPredict applies a second round of more detailed “expensive predictions”
to approximate Shortest Remaining Processing Time for these jobs. Our analysis takes into account the cost of
prediction. We examine the effect of this cost for two distinct models. In the external cost model, predictions
are generated by some external method without impacting job service times but incur a cost. In the server time
cost model, predictions themselves require server processing time, and are scheduled on the same server as the
jobs.
</div>
</br>
10:30-11 <b>Nina Balcan</b>:</br>
11-11:30 Discussion/break</br>
11:30-12 <b>Aditya Bhaskara</b>: Online Learning and Bandits with Hints [<a href="javascript:toggle('Bhaskara')">abstract</a>]
<div id="Bhaskara" style="display: none">
<p id="abs">
We consider variants of online convex optimization in which before choosing a point, 
the algorithm receives a "hint" about the loss function that is to arrive. Such hints may be the outcome of 
some prediction algorithms operating with domain-specific side information. Recent works have shown that if 
hints are guaranteed to be "well-correlated" with the loss, then known regret bounds can be improved significantly 
(under additional assumptions).
</br></br>
In this talk, I will discuss some limitations of existing results and present hint models that can overcome them. 
Specifically, for the simple setting of online linear optimization on the sphere with bandit feedback, 
I will show a lower bound that \sqrt{T} regret is unavoidable even with well-correlated hints. 
I will then discuss the surprising power of "queried" hints: if the algorithm can get a weak indication of 
which of two points (or arms) is better before playing, it can overcome the lower bound and obtain constant regret. 
</div>
</br>
12-1 Lightning Talks</br>
1-2 Lunch</br>
2-2:30 <b>Ben Moseley</b>:</br>
2:30-3 <b>Sergei Vassilvitskii</b>:</br>
3-3:30 Discussion/break</br>
3:30-4 <b>Sami Davies</b>:</br>
4-4:30 <b>Huy L. Nguyen</b>:</br>
4:30-5:30 Poster session</br>
</br>
<u>Tuesday:</u></br>
9:30-10 Breakfast</br>
10-10:30 <b>Anupam Gupta</b>: Graph Exploration with Predictions [<a href="javascript:toggle('Gupta')">abstract</a>]
<div id="Gupta" style="display: none">
<p id="abs">
The problem of exploring an unknown graph using predictions is a classical one, and strategies like Astar have 
long been studied for it. I will discuss some recent quantitative results about this problem, 
which give bounds on the cost of an agent exploring the graph in terms of the quality of the predictions. 
This is based on joint work with Siddhartha Banerjee, Vincent Cohen-Addad, and Zhouzi Li. 
</div>
</br>
10:30-11 <b>Quanquan Liu</b>:</br>
11-11:30 Discussion/break</br>
11:30-12 <b>Ravi Kumar</b>:</br>
12-1 Lightning Talks</br>
1-2 Lunch</br>
2-2:30 <b>David Woodruff</b>:</br>
2:30-3 <b>Barna Saha</b>:</br>
3-3:30 Discussion/break</br>
3:30-4 <b>Adam Wierman</b>:</br>
4-4:30 <b>Yossi Azar</b>:</br>
4:30-5:30 Open problem session</br>
</br>
<u>Wednesday:</u></br>
9:30-10 Breakfast</br>
10-10:30 <b>Tim Kraska</b>:</br>
10:30-11 <b>Negin Golrezaei</b>:</br>
11-11:30 Discussion/break</br>
11:30-12 <b>Adam Polak</b>: Approximation Algorithms with Predictions [<a href="javascript:toggle('Polak')">abstract</a>]
<div id="Polak" style="display: none">
<p id="abs">
How can one use predictions to improve over approximation guarantees of classic algorithms, without increasing the running time? 
We propose a generic method for a broad class of optimization problems that ask to select a feasible subset of input items of 
minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, 
Minimum Weight Perfect Matching, Knapsack, and Clique. Our algorithms produce an optimal solution when provided with perfect 
predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error 
we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the 
NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems 
as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; 
we demonstrate this on the Steiner Tree problem. This is joint work with Antonios Antoniadis, Marek Eliáš, and Moritz Venzin.
</div>
</br>
12-12:30 <b>Debmalya Panigrahi</b>: Learning-augmented Assignment: Santa Claus does Load Balancing [<a href="javascript:toggle('Panigrahi')">abstract</a>]
<div id="Panigrahi" style="display: none">
<p id="abs">
Assignment problems are among the most well-studied in online algorithms. 
In these problems, a sequence of items arriving online must be assigned among a set of 
agents so as to optimize a given objective. This encompasses scheduling problems for minimizing makespan, 
p-norms, and other objectives, as well as fair division problems such as the Santa Claus problem and 
Nash welfare maximization. One common feature is that many of these problems are characterized by 
strong worst-case lower bounds in the online setting. To circumvent these impossibility results, 
recent research has focused on using additional (learned) information about the problem instance 
and this has led to dramatic improvements in the competitive ratio over the worst case. 
In this talk, I will first survey some of this literature (Lattanzi et al., SODA 20; Li and Xian, ICML 21; 
Banerjee et al., SODA 22; Barman et al., AAAI 22) that addresses specific problems in this domain. 
I will then proceed to describe joint work with Ilan Cohen that brings these problems under one umbrella: 
we give a single algorithmic framework for near-optimal learning-augmented online assignment for a 
large class of maximization and minimization objectives, including all the problems mentioned above. 
</div>
</br>
12:30-1:30 Lunch</br>
1:30-2 <b>Sungjin Im</b>:</br>
2-2:30 <b>Ellen Vitercik</b>: Online Matching with Graph Neural Networks [<a href="javascript:toggle('Vitercik')">abstract</a>]
<div id="Vitercik" style="display: none">
<p id="abs">
Online Bayesian bipartite matching is a central problem in digital marketplaces and exchanges, 
including advertising, crowdsourcing, ridesharing, and kidney exchange. We introduce a graph neural network 
(GNN) approach that emulates the problem's combinatorially complex optimal online algorithm, which selects actions 
(e.g., which nodes to match) by computing each action's value-to-go (VTG)—the expected weight of the final matching if the algorithm takes that action, 
then acts optimally in the future. We train a GNN to estimate VTG and show empirically that this GNN returns high-weight matchings across a variety of tasks. 
Moreover, we identify a common family of graph distributions in spatial crowdsourcing applications, such as rideshare, 
under which VTG can be efficiently approximated by aggregating information within local neighborhoods in the graphs. 
This structure matches the local behavior of GNNs, providing theoretical justification for our approach.
</br></br>
This is joint work with Alexandre Hayderi, Amin Saberi, and Anders Wikum.
</div>
</br>
2:30-3 <b>Eric Balkanski</b>: Online Mechanism Design with Predictions [<a href="javascript:toggle('Balkanski')">abstract</a>]
<div id="Balkanski" style="display: none">
<p id="abs">
Aiming to overcome some of the limitations of worst-case analysis, the recently proposed framework of ``algorithms with predictions'' 
allows algorithms to be augmented with a (possibly erroneous) machine-learned prediction that they can use as a guide. In this framework, 
the goal is to obtain improved guarantees when the prediction is correct, which is called consistency, while simultaneously guaranteeing 
some worst-case bounds even when the prediction is arbitrarily wrong, which is called robustness. The vast majority of the work on this framework has 
focused on a refined analysis of online algorithms augmented with predictions regarding the future input. A subsequent line of work has also 
successfully adapted this framework to mechanism design, where the prediction is regarding the private information of strategic agents. 
In this paper, we initiate the study of online mechanism design with predictions, which combines the challenges of online algorithms with 
predictions and mechanism design with predictions.
</br>
</br>
We consider the well-studied problem of designing a revenue-maximizing auction to sell a single item to strategic bidders who arrive and 
depart over time, each with an unknown, private, value for the item. We study the learning-augmented version of this problem where the 
auction designer is given a prediction regarding the maximum value over all agents. Our main result is a strategyproof mechanism whose 
revenue guarantees are alpha-consistent with respect to the highest value and (1-alpha^2)/4-robust with respect to the second-highest value, 
for alpha in [0,1]. We show that this trade-off is optimal within a broad and natural family of auctions, meaning that any $\alpha$-consistent 
mechanism in that family has robustness at most (1-alpha^2)/4. Finally, we extend our mechanism to also obtain expected revenue 
that is proportional to the prediction quality.
</div>
</br>


</body>
</html>
