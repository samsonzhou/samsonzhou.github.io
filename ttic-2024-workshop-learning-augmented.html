<html>
<head>
<title>
Workshop on Learning-Augmented Algorithms	
</title>

	<script language="JavaScript" type="text/JavaScript">
	function toggle(id) {
	if (document.getElementById){
		if (document.getElementById(id).style.display == "none"){
			document.getElementById(id).style.display = "flex";
			}
		else{
			document.getElementById(id).style.display = "none";
			}
		}
	}
	</script>
</head>
<body>
<h2>
<r>
Workshop on Learning-Augmented Algorithms
</r>
</h2>
<h3>
<r>
Date: August 19-21, 2024</br>
Location: TTIC, Chicago, IL</br>
Address: 6045 S Kenwood Ave, Chicago, IL 60637
</r>
<section>
			<p align="left">
			<iframe loading="lazy"
					src="https://www.google.com/maps?q=Toyota+Technological+Institute+at+Chicago&#038;t=m&#038;z=14&#038;output=embed&#038;iwloc=near"
					title="TTIC"
					aria-label="TTIC"
					style="width:30%; height:200px;overflow:auto;"
			></iframe>
			</p>
</section>
<a href="https://samsonzhou.github.io/Getting-to-TTIC-Maps.pdf">Parking Information</a>		
</h3>
<h3>
Organizers:
</h3>
<ul>
<li><a href="https://people.csail.mit.edu/indyk/">Piotr Indyk</a>, MIT</li>
<li><a href="http://www.mit.edu/~vakilian/">Ali Vakilian</a>, TTIC</li>
<li><a href="https://samsonzhou.github.io/">Samson Zhou</a>, Texas A&#38;M</li>
</ul>
<h3>
Sponsors:
</h3>
This workshop is part of the <a href="https://www.ttic.edu/summer-workshop-2024/">2024 TTIC Summer Workshop Program</a> and is sponsored by generous support from TTIC and NSF. 
<h3>
Registration:
</h3>
<a href="https://docs.google.com/forms/d/e/1FAIpQLSecfgYl1s4dQgJt3nMr5vmKX6PhVGgCHHkorzcJWrIb0TTPHw/viewform">Registration Form</a>
</br>
</br>
<i>
We will be holding sessions for poster and lightning talks. 
If you are interested in presenting a poster or giving a lightning talk, please indicate your interest and preference in the registration form. 
While we aim to accommodate all submissions, we may have to limit the number of presentations due to time and space constraints
</i>
<h3>
Participants:
</h3>
<ul>
<li><a href="https://www.cs.tau.ac.il/~azar/">Yossi Azar</a> (Tel Aviv)</li>
<li><a href="https://users.cs.utah.edu/~bhaskara/">Aditya Bhaskara</a> (Utah)</li>
<li><a href="https://www.cs.cmu.edu/~ninamf/">Nina Balcan</a> (CMU)</li>
<li><a href="https://ericbalkanski.com/">Eric Balkanski</a> (Columbia)</li>
<li><a href="https://samidavies.com">Sami Davies</a> (Simons)</li>
<li><a href="https://www.mit.edu/~golrezae/">Negin Golrezaei</a> (MIT)</li>
<li><a href="https://cs.nyu.edu/~anupamg/">Anupam Gupta</a> (NYU)</li>
<li><a href="https://faculty.ucmerced.edu/sim3/">Sungjin Im</a> (UC Merced)</li>
<li><a href="https://people.csail.mit.edu/indyk/">Piotr Indyk</a> (MIT)</li>
<li><a href="https://people.csail.mit.edu/kraska/">Tim Kraska</a> (MIT)</li>
<li><a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a> (Google)</li>
<li><a href="https://quanquancliu.com/">Quanquan C. Liu</a> (Yale)</li>
<li><a href="https://www.eecs.harvard.edu/~michaelm/">Michael Mitzenmacher</a> (Harvard)</li>
<li><a href="https://www.andrew.cmu.edu/user/moseleyb/">Ben Moseley</a> (CMU)</li>
<li><a href="https://www.ccs.neu.edu/home/hlnguyen/">Huy L. Nguyen</a> (Northeastern)</li>
<li><a href="https://www.debmalyapanigrahi.org/">Debmalya Panigrahi</a> (Duke)</li>
<li><a href="https://adampolak.github.io/">Adam Polak</a> (Bocconi)</li>
<li><a href="https://barnasaha.net/">Barna Saha</a> (UC San Diego)</li>
<li><a href="http://www.mit.edu/~vakilian/">Ali Vakilian</a> (TTIC)</li>
<li><a href="https://theory.stanford.edu/~sergei/">Sergei Vassilvitskii</a> (Google)</li>
<li><a href="https://vitercik.github.io/">Ellen Vitercik</a> (Stanford)</li>
<li><a href="https://adamwierman.com/">Adam Wierman</a> (Caltech)</li>
<li><a href="https://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a> (CMU)</li>
<li><a href="https://samsonzhou.github.io/">Samson Zhou</a> (Texas A&#38;M)</li>
</ul>
<h3>
Schedule (Tentative):
</h3>
<u>Monday:</u> <a href="https://uchicago.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=96281c01-9be9-4473-a424-b1cd0043a372">Day 1 Link</a></br>
9:30-9:50 Breakfast</br>
9:50-10 Opening Remarks</br>
10-10:30 <b>Michael Mitzenmacher</b>: SkipPredict: When to Invest in Predictions for Scheduling [<a href="javascript:toggle('Mitzenmacher')">abstract</a>]
<div id="Mitzenmacher" style="display: none">
<p id="abs">
In light of recent work on scheduling with predicted job sizes, we consider the effect of the cost of predictions
in queueing systems, removing the assumption in prior research that predictions are external to the system’s
resources and/or cost-free. In particular, we introduce a novel approach to utilizing predictions, SkipPredict,
designed to address their inherent cost. Rather than uniformly applying predictions to all jobs, we propose
a tailored approach that categorizes jobs based on their prediction requirements. To achieve this, we employ
one-bit “cheap predictions” to classify jobs as either short or long. SkipPredict prioritizes predicted short jobs
over long jobs, and for the latter, SkipPredict applies a second round of more detailed “expensive predictions”
to approximate Shortest Remaining Processing Time for these jobs. Our analysis takes into account the cost of
prediction. We examine the effect of this cost for two distinct models. In the external cost model, predictions
are generated by some external method without impacting job service times but incur a cost. In the server time
cost model, predictions themselves require server processing time, and are scheduled on the same server as the
jobs.
</div>
</br>
10:30-11 <b>Nina Balcan</b>: Learning Machine Learning Algorithms [<a href="javascript:toggle('Balcan')">abstract</a>]
<div id="Balcan" style="display: none">
<p id="abs">
Recent advances in machine learning have changed the theory and practice of algorithm design and analysis. 
From a theoretical perspective, new analysis frameworks have emerged including data-driven algorithm design 
(where we learn algorithms for solving problem instances coming from a given domain based on training instances from that domain) 
and algorithms with predictions (where algorithms are designed to take advantage of possibly 
imperfect machine learning predictions of some aspect of the problem).
</br>
</br>
In this talk we argue that we also need to loop back and expand our techniques for designing and 
analyzing machine learning algorithms themselves. I will present recent work that builds on tools 
from data driven algorithm design to address the question of how to tune machine learning algorithms, 
to achieve desired performance for a given domain, with provable guarantees. The main case study I will discuss 
is learning decision tree learning algorithms, a class of machine learning approaches often preferred 
in practice due to their high degree of interpretability.
</div>
</br>
11-11:30 Discussion/break</br>
11:30-12 <b>Aditya Bhaskara</b>: Online Learning and Bandits with Hints [<a href="javascript:toggle('Bhaskara')">abstract</a>]
<div id="Bhaskara" style="display: none">
<p id="abs">
We consider variants of online convex optimization in which before choosing a point, 
the algorithm receives a "hint" about the loss function that is to arrive. Such hints may be the outcome of 
some prediction algorithms operating with domain-specific side information. Recent works have shown that if 
hints are guaranteed to be "well-correlated" with the loss, then known regret bounds can be improved significantly 
(under additional assumptions).
</br></br>
In this talk, I will discuss some limitations of existing results and present hint models that can overcome them. 
Specifically, for the simple setting of online linear optimization on the sphere with bandit feedback, 
I will show a lower bound that \sqrt{T} regret is unavoidable even with well-correlated hints. 
I will then discuss the surprising power of "queried" hints: if the algorithm can get a weak indication of 
which of two points (or arms) is better before playing, it can overcome the lower bound and obtain constant regret. 
</div>
</br>
12-1 Lightning Talks</br>
<ul style="margin-top: 0px; line-height: 1em; margin-bottom: 0px">
<li>
Maoyuan Song (Purdue)
<li>
Vaidehi Srinivas (Northwestern)
<li>
Davin Choo (NUS)
<li>
Pooja Kulkarni (UIUC)
</li>
</ul>
1-2 Lunch</br>
2-2:30 <b>Ben Moseley</b>: Incremental Topological Ordering and Cycle Detection with Predictions [<a href="javascript:toggle('Moseley')">abstract</a>]
<div id="Moseley" style="display: none">
<p id="abs">
This talk discusses how to leverage the framework of algorithms-with-predictions to design data structures for two fundamental dynamic graph problems: 
incremental topological ordering and cycle detection. In these problems, the input is a directed graph on n nodes, and the m edges arrive one by one. 
The data structure must maintain a topological ordering of the vertices at all times and detect if the newly inserted edge creates a cycle. 
The theoretically best worst-case algorithms for these problems have high update cost (polynomial in n and m). In practice, greedy heuristics 
(that recompute the solution from scratch each time) perform well but can have high update cost in the worst case.
</br>
</br>
We bridge this gap by leveraging predictions to design a learned new data structure for the problems. Our data structure guarantees consistency, 
robustness, and smoothness with respect to predictions -- that is, it has the best possible running time under perfect predictions, 
never performs worse than the best-known worst-case methods, and its running time degrades smoothly with the prediction error. 
Moreover, we demonstrate empirically that predictions, learned from a very small training dataset, are sufficient to provide significant speed-ups on real datasets.
</div>
</br>
2:30-3 <b>Sergei Vassilvitskii</b>:</br>
3-3:30 Discussion/break</br>
3:30-4 <b>Sami Davies</b>:</br>
4-4:30 <b>Huy L. Nguyen</b>: Improved Frequency Estimation Algorithms with and without Predictions [<a href="javascript:toggle('Nguyen')">abstract</a>]
<div id="Nguyen" style="display: none">
<p id="abs">
Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. 
Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that 
probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al. (2019) 
introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. 
In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements 
will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms 
the learning based algorithm of Hsu et al. without the use of any predictions. Augmenting our algorithm with heavy-hitter predictions 
further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all 
experiments compared to prior approaches. This is joint work with Anders Aamand, Justin Y. Chen, Sandeep Silwal, and Ali Vakilian.
</div>
</br>
4:30-5:30 Poster session</br>
</br>


<u>Tuesday:</u> <a href="https://uchicago.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=b9e4facf-8fc9-4a56-a2f6-b1cd0043a3bd">Day 2 Link</a></br>
9:30-10 Breakfast</br>
10-10:30 <b>Anupam Gupta</b>: Graph Exploration with Predictions [<a href="javascript:toggle('Gupta')">abstract</a>]
<div id="Gupta" style="display: none">
<p id="abs">
The problem of exploring an unknown graph using predictions is a classical one, and strategies like Astar have 
long been studied for it. I will discuss some recent quantitative results about this problem, 
which give bounds on the cost of an agent exploring the graph in terms of the quality of the predictions. 
This is based on joint work with Siddhartha Banerjee, Vincent Cohen-Addad, and Zhouzi Li. 
</div>
</br>
10:30-11 <b>Quanquan Liu</b>:</br>
11-11:30 Discussion/break</br>
11:30-12 <b>Ravi Kumar</b>:</br>
12-1 Lightning Talks</br>
<ul style="margin-top: 0px; line-height: 1em; margin-bottom: 0px">
<li>
Erasmo Tani (UChicago)
<li>
Eniko Kevi (Universite Grenoble Alpes, Cornell)
<li>
Dravyansh Sharma (CMU)
<li>
Vipul Arora (NUS)
<li>
Omer Wasim (Northeastern)
</ul>
1-2 Lunch</br>
2-2:30 <b>David Woodruff</b>:</br>
2:30-3 <b>Barna Saha</b>: Clustering with Queries [<a href="javascript:toggle('Saha')">abstract</a>]
<div id="Saha" style="display: none">
<p id="abs">
Given a ground truth clustering, we consider the problem of recovering the underlying clusters by querying an oracle. 
The oracle receives as its input a subset of nodes, and outputs the number of clusters induced on those nodes. 
We would like to minimize the number of queries to the oracle, and consider further practical considerations such as query size, 
adaptivity etc. We will dig into the history of this problem, the motivation to study it, and will present some recent developments in this area.
</div>
</br>
3-3:30 Discussion/break</br>
3:30-4 <b>Adam Wierman</b>: Learning Augmented Algorithms for MDPs [<a href="javascript:toggle('Wierman')">abstract</a>]
<div id="Wierman" style="display: none">
<p id="abs">
Recent advances in machine learning have changed the theory and practice of algorithm design and analysis. From a theoretical perspective, 
new analysis frameworks have emerged including data-driven algorithm design (where we learn algorithms for solving problem instances coming 
from a given domain based on training instances from that domain) and algorithms with predictions (where algorithms are designed to take 
advantage of possibly imperfect machine learning predictions of some aspect of the problem).
</br>
</br>
In this talk we argue that we also need to loop back and expand our techniques for designing and analyzing machine learning algorithms 
themselves. I will present recent work that builds on tools from data driven algorithm design to address the question of how to tune 
machine learning algorithms, to achieve desired performance for a given domain, with provable guarantees. The main case study I will 
discuss is learning decision tree learning algorithms, a class of machine learning approaches often preferred in practice due to their 
high degree of interpretability.
</div>
</br>
4-4:30 <b>Yossi Azar</b>: Online predictions and close to 1-smoothness [<a href="javascript:toggle('Azar')">abstract</a>]
<div id="Azar" style="display: none">
<p id="abs">
We consider learning augmented problems, where our goal is to be within 1% of an algorithm that "follows the prediction". 
Our prediction is provided at any step of an online algorithm and hence may be adaptive and based possibly upon all previous history. 
Specifically, the prediction may be any information that yields an action to be taken by an online algorithm at the current step. 
An algorithm is called 1-smooth if its cost (value) is as low (high) as the algorithm that follows the prediction.
For the packet scheduling with deadlines problem (where the goal is to maximize the total value of transmitted packets), 
we provide 1-smooth and 3-robust algorithm. That means an algorithm that always achieves a value which is as large as 
"following the prediction" but never worse than 3 times the optimal solution for every possible sequence and prediction.
For the list update problem (where the goal is to minimize the access cost plus the movement cost), 
we provide (1+\epsilon)-smooth randomized algorithm, offering robustness of O(1/\epsilon^4). 
That means an algorithm that always pays a cost which is within (1+\epsilon) of the cost incurred by 
"following the prediction" but never worse than O(1/\epsilon^4) times the optimal solution for every possible sequence and prediction.
</br>
</br>
Joint work with Shahar Lewkowicz, Varun Suriyanarayana and Or Vardi
</div>
</br>
4:30-5:30 Open problem session</br>
</br>


<u>Wednesday:</u> <a href="https://uchicago.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=f4895fa9-e6e2-4430-8f65-b1cd0043a3db">Day 3 Link</a></br>
9:30-10 Breakfast</br>
10-10:30 <b>Tim Kraska</b>:</br>
10:30-11 <b>Negin Golrezaei</b>:</br>
11-11:30 Discussion/break</br>
11:30-12 <b>Adam Polak</b>: Approximation Algorithms with Predictions [<a href="javascript:toggle('Polak')">abstract</a>]
<div id="Polak" style="display: none">
<p id="abs">
How can one use predictions to improve over approximation guarantees of classic algorithms, without increasing the running time? 
We propose a generic method for a broad class of optimization problems that ask to select a feasible subset of input items of 
minimal (or maximal) total weight. This gives simple (near-)linear-time algorithms for, e.g., Vertex Cover, Steiner Tree, 
Minimum Weight Perfect Matching, Knapsack, and Clique. Our algorithms produce an optimal solution when provided with perfect 
predictions and their approximation ratio smoothly degrades with increasing prediction error. With small enough prediction error 
we achieve approximation guarantees that are beyond the reach without predictions in given time bounds, as exemplified by the 
NP-hardness and APX-hardness of many of the above problems. Although we show our approach to be optimal for this class of problems 
as a whole, there is a potential for exploiting specific structural properties of individual problems to obtain improved bounds; 
we demonstrate this on the Steiner Tree problem. This is joint work with Antonios Antoniadis, Marek Eliáš, and Moritz Venzin.
</div>
</br>
12-12:30 <b>Debmalya Panigrahi</b>: Learning-augmented Assignment: Santa Claus does Load Balancing [<a href="javascript:toggle('Panigrahi')">abstract</a>]
<div id="Panigrahi" style="display: none">
<p id="abs">
Assignment problems are among the most well-studied in online algorithms. 
In these problems, a sequence of items arriving online must be assigned among a set of 
agents so as to optimize a given objective. This encompasses scheduling problems for minimizing makespan, 
p-norms, and other objectives, as well as fair division problems such as the Santa Claus problem and 
Nash welfare maximization. One common feature is that many of these problems are characterized by 
strong worst-case lower bounds in the online setting. To circumvent these impossibility results, 
recent research has focused on using additional (learned) information about the problem instance 
and this has led to dramatic improvements in the competitive ratio over the worst case. 
In this talk, I will first survey some of this literature (Lattanzi et al., SODA 20; Li and Xian, ICML 21; 
Banerjee et al., SODA 22; Barman et al., AAAI 22) that addresses specific problems in this domain. 
I will then proceed to describe joint work with Ilan Cohen that brings these problems under one umbrella: 
we give a single algorithmic framework for near-optimal learning-augmented online assignment for a 
large class of maximization and minimization objectives, including all the problems mentioned above. 
</div>
</br>
12:30-1:30 Lunch</br>
1:30-2 <b>Sungjin Im</b>: Online Load and Graph Balancing for Random Order Inputs [<a href="javascript:toggle('Im')">abstract</a>]
<div id="Im" style="display: none">
<p id="abs">
Online load balancing for heterogeneous machines aims to minimize the makespan 
(maximum machine workload) by scheduling arriving jobs with varying sizes on different machines. In the adversarial setting, 
where an adversary chooses not only the collection of job sizes but also their arrival order, 
the problem is well-understood and the optimal competitive ratio is known to be $\Theta(\log m)$ where $m$ is the number of machines. 
In the more realistic random arrival order model, the understanding is limited. Previously, the best lower bound on the 
competitive ratio was only $\Omega(\log \log m)$. We significantly improve this bound by showing an $\Omega(\sqrt {\log m})$ lower bound, 
even for the restricted case where each job has a unit size on two machines and infinite size on the others. On the positive side, 
we propose an $O(\log m/ \log \log m)$-competitive algorithm, demonstrating that better performance is possible in the random arrival model.
</br></br>
This is a joint work with Ravi Kumar, Shi Li, Aditya Petety, and Manish Purohit, which appeared in SPAA '24.
</div>
</br>
2-2:30 <b>Ellen Vitercik</b>: Online Matching with Graph Neural Networks [<a href="javascript:toggle('Vitercik')">abstract</a>]
<div id="Vitercik" style="display: none">
<p id="abs">
Online Bayesian bipartite matching is a central problem in digital marketplaces and exchanges, 
including advertising, crowdsourcing, ridesharing, and kidney exchange. We introduce a graph neural network 
(GNN) approach that emulates the problem's combinatorially complex optimal online algorithm, which selects actions 
(e.g., which nodes to match) by computing each action's value-to-go (VTG)—the expected weight of the final matching if the algorithm takes that action, 
then acts optimally in the future. We train a GNN to estimate VTG and show empirically that this GNN returns high-weight matchings across a variety of tasks. 
Moreover, we identify a common family of graph distributions in spatial crowdsourcing applications, such as rideshare, 
under which VTG can be efficiently approximated by aggregating information within local neighborhoods in the graphs. 
This structure matches the local behavior of GNNs, providing theoretical justification for our approach.
</br></br>
This is joint work with Alexandre Hayderi, Amin Saberi, and Anders Wikum.
</div>
</br>
2:30-3 <b>Eric Balkanski</b>: Online Mechanism Design with Predictions [<a href="javascript:toggle('Balkanski')">abstract</a>]
<div id="Balkanski" style="display: none">
<p id="abs">
Aiming to overcome some of the limitations of worst-case analysis, the recently proposed framework of ``algorithms with predictions'' 
allows algorithms to be augmented with a (possibly erroneous) machine-learned prediction that they can use as a guide. In this framework, 
the goal is to obtain improved guarantees when the prediction is correct, which is called consistency, while simultaneously guaranteeing 
some worst-case bounds even when the prediction is arbitrarily wrong, which is called robustness. The vast majority of the work on this framework has 
focused on a refined analysis of online algorithms augmented with predictions regarding the future input. A subsequent line of work has also 
successfully adapted this framework to mechanism design, where the prediction is regarding the private information of strategic agents. 
In this paper, we initiate the study of online mechanism design with predictions, which combines the challenges of online algorithms with 
predictions and mechanism design with predictions.
</br>
</br>
We consider the well-studied problem of designing a revenue-maximizing auction to sell a single item to strategic bidders who arrive and 
depart over time, each with an unknown, private, value for the item. We study the learning-augmented version of this problem where the 
auction designer is given a prediction regarding the maximum value over all agents. Our main result is a strategyproof mechanism whose 
revenue guarantees are alpha-consistent with respect to the highest value and (1-alpha^2)/4-robust with respect to the second-highest value, 
for alpha in [0,1]. We show that this trade-off is optimal within a broad and natural family of auctions, meaning that any $\alpha$-consistent 
mechanism in that family has robustness at most (1-alpha^2)/4. Finally, we extend our mechanism to also obtain expected revenue 
that is proportional to the prediction quality.
</div>
</br>


</body>
</html>
